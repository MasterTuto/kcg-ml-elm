{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOzTxlld0XVT8pS7qzXxpDv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Linear Regression using Least Squares Method\n","\n","##Usage\n","\n","It is used to find a line that best fit an amount of points distributed in space.\n","\n","![Graphic1](https://www.scribbr.com/wp-content/uploads/2021/04/explanatory-and-response-variables-768x432.png)\n","\n","In the example above, it is possible to see that the line fits best most points at the same time. This will minimize MSE, because MSE is defined as $\\frac{1}{n}\\sum(Y_1 - Y_0)^2$, which is literally the average size of the distance with all points' $Y_1$ and $Y_0$ similar to the Euclidian Norm.\n","\n","##Calculation\n","\n","Where:\n","\n","* $\\beta$ is the vector of coefficients that minimize the MSE.\n","* $X$ is the matrix of predictor variables in the form of a design matrix. Each line is an object, and each column is a variable. The first column represents the intercept term. Usually filled with 1's for estimation of Y-intercept. \n","* $X^T$ is $X$ transposed.\n","* $Y$ is the vector of response variables.\n","\n","##Making an example\n","\n","The following graphic represent points of the matrix:\n","\n","Points $= [1, 2, 3, 4, 5, 6, 7]$\n","\n","$Y = [1.5, 3.8, 6.7, 9.0, 11.2, 13.6, 16]$\n","\n","![graphic2](https://i.ibb.co/SnCLTd2/figure1.png)\n","\n","First, build the $X$ design matrix (the first matrix below) filling the intercept terms with ones, creating the matrix $X$. We can also calculate $X^T$.\n","\n","![matrixXandTransposed](https://i.ibb.co/kq5YVXS/image.png)\n","\n","Calculating $X^TX$:\n","\n","![MatrixProduct](https://i.ibb.co/kSkXC1k/image.png)\n","\n","Then it's inverse will be:\n","\n","![MatrixInverse](https://i.ibb.co/PMwLqwp/image.png)\n","\n","Now that we already have $(X^TX)^{-1}$, remember, $\\beta = (X^TX)^{-1}X^TY$. So, multiplying it by $X^T$:\n","\n","![MatrixProductfromInversed](https://i.ibb.co/R7vNzr5/image.png)\n","\n","In the end, multiplying by the response variables:\n","\n","![MatrixResult](https://i.ibb.co/DQbRzN9/image.png)\n","\n","Those are the coefficients that minimize the MSE. Remember the linear equation formula, $y = mx + b$. Here, $-\\frac{29}{35} = b$ and $\\frac{169}{70} = m$. This makes gets us to $y = \\frac{169}{70}x - \\frac{29}{35}$. Let's see how this looks on the graphic:\n","\n","![FinalChart](https://i.ibb.co/1fhWVFH/figure1.png)\n","\n","#Using the Penrose-Inverse\n","\n","The Penrose-Inverse formula is defined as:\n","\n","$A^{+} = (A^TA)^{-1}A^T$\n","\n","* $A^{+}$ is the Moore-Penrose Inverse.\n","* $A$ is the original matrix.\n","* $A^T$ is $A$ transposed.\n","\n","Putting side by side with $\\beta = (X^TX)^{-1}X^TY$, notice that $(X^TX)^{-1}X^T$ is replaceable with the Moore-Penrose Inverse $X^{+}$.\n","\n","With Moore-Penrose Inverse: $\\beta = X^{+}Y$\n"],"metadata":{"id":"QfFAXqdDp20w"}},{"cell_type":"code","source":["import numpy as np\n","\n","A = np.array([[1, 2], [3, 4]]) #example matrix\n","At = A.transpose()\n","\n","mooreA = np.dot(np.linalg.inv(np.dot(At, A)), At) #the math way\n","\n","print(mooreA) #print the matrix\n","print(np.linalg.pinv(A)) #the numpy way\n","#prints exactly the same answer"],"metadata":{"id":"DzN5sPgEM1u5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##References\n","\n","https://www.youtube.com/watch?v=P8hT5nDai6A -> This video explains the same Linear Regression model, but shows a different way to do it. He does the same example we do in this article, but we do it in Matrix Form, which involves the Penrose-Inverse. https://www.ma.imperial.ac.uk/~das01/GSACourse/Regression.pdf -> This article explains more on this method we are using here."],"metadata":{"id":"ZKod-YO95KWx"}}]}